\chapter{Foundations}

A statistical Hypothesis is a statement or assumption about the parameters of one or more populations, which may be true or false. Each such truth value can be expressed in the form of a hypothesis, usually denoted as $H_0$, called the null hypothesis, and an alternative hypothesis, usually denoted as $H_1$ or Halt, which is its complement.

\section{Theory of Sampling}
\subsection{Terminology}
\begin{enumerate}
 \item A population is a set $\{x_i\}_{i=1}^N$ of elements belonging to the object under study, which need to be observed, collected, and analyzed based on certain characteristics. A parameter is a numerical description of a population.
 \item A sample is a subset $\{x_{i_j}\}_{j=1}^n$ of units selected from the population according to a certain sampling method. A statistic is a numerical description of a sample.
 \item Let the population belong to a distribution represented by a random variable $X$. A random sample is a collection $\{X_i\}_{i=1}^n$ such that $X,X_1,\ldots,X_n$ are independent and identically distributed.
\end{enumerate}

\subsection{Statistical characteristics}

Statistical characteristics are important properties on a population, a sample or a random sample directly related to the content of the study and survey, which need to be collected from the units of the population, generally divided into two groups

\begin{enumerate}
 \item Central tendency includes mean, median, mode, giving us an idea of the trend of the data points clustering around certain values.
 \item Dispersion includes variance, standard deviation, which describe the spread or variability of the data points around the central tendency.
\end{enumerate}

In details, these quantities are formulated as followings.
\begin{enumerate}
 \item Population mean
       \begin{equation}
        \mu = \frac{1}{N} \sum_{i=1}^N x_i.
       \end{equation}
 \item Sample mean
       \begin{equation}
        \bar{x} = \frac{1}{n} \sum_{j=1}^n x_{i_j}.
       \end{equation}
 \item Random sample mean
       \begin{equation}
        \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i.
       \end{equation}
 \item Population variance
       \begin{equation}
        \sigma^2 = \dfrac{1}{N} \sum_{i=1}^N (x_i - \mu)^2.
       \end{equation}
 \item Sample variance
       \begin{equation}
        s^2 = \dfrac{1}{n-1} \sum_{j=1}^n (x_{i_j} - \bar{x})^2.
       \end{equation}
       The quantity $s^2$ is also called corrected sample variance using Bessel's correction, because the denominator is $n-1$, instead of $n$. This is briefly explained as that since $$\mu = \argmin\limits_{y} \sum_{i=1}^N (x_i - y)^2 \text{ and } \bar{x} = \argmin\limits_{y} \sum_{i=1}^n (x_{i_j} - y)^2,$$ we have
       \begin{align*}
        \dfrac{1}{n} \sum_{j=1}^n (x_{i_j} - \bar{x})^2
         & \le \dfrac{1}{n} \sum_{j=1}^n (x_{i_j} - \mu)^2                                               \\
         & = \dfrac{1}{N} \sum_{j=1}^n (x_{i} - \mu)^2.    & \text{(assume that the sample is unbiased)}
       \end{align*}
       Hence, dividing by $n-1$ slightly increases the sample variance to better estimate the population variance.
 \item Random sample variance
       \begin{equation}
        S^2 = \dfrac{1}{N-1} \sum_{n=1}^n (X_i - \bar{X})^2.
       \end{equation}
 \item Population, sample and random sample derivations are the square roots $\sigma$, $s$ and $S$, respectively.

 \item Sample ratio
       \begin{equation}
        f = \dfrac{n}{N}
       \end{equation}
 \item Coefficient of variation
       \begin{equation}
        \mathrm{CV} = \left(\frac{s}{\bar{x}}\right) \cdot 100\%
       \end{equation}
 \item Suppose that the sample $\{x_{i_j}\}_{j=1}^n$ is in increasing order. The sample median is calculated as
       \begin{equation}
        \mathrm{med} = \begin{cases}
         x_{i_{k+1}},                            & \text{ if } n = 2k+1 \\
         \dfrac{1}{2} (x_{i_{k}} + x_{i_{k+1}}), & \text{ if } n = 2k.
        \end{cases}
       \end{equation}
 \item Quartiles include three values $Q_1,Q_2$ and $Q_3$. Where $Q_2$ is the median, $Q_1$ and $Q_3$ is the median of the set of samples less than and more than $Q_2$, respectively. The quantity $\mathrm{IQR} = Q_3-Q_1$ is called the interquartile range.
 \item An outlier is a point outsize the range $(Q1 - 1.5 \times \mathrm{IQR}, Q3 + 1.5 \times \mathrm{IQR})$.
\end{enumerate}

\section{Statistical Hypothesis Testing}
The process of testing a statistical hypothesis is a standardized procedure for making decisions to reject or not reject a hypothesis based on sample data. This process is called hypothesis testing and typically involves four steps:

\begin{enumerate}
 \item \textbf{Setting Hypotheses} states the null hypothesis ($H_0$) and the  alternative hypothesis ($H_0$). The hypotheses are formulated in a mutually exclusive manner, meaning if one is true, the other must be false.
 \item \textbf{Constructing an Analysis Plan} outlines how sample data will be used to evaluate the null hypothesis. Evaluation typically centers around a single test statistic.
 \item \textbf{Analyzing Sample Data} determines the values of the sample statistic (mean, proportion, $t$-statistic, $z$-score, etc.) as described in the analysis plan.
 \item \textbf{Interpreting Results} applies decision rules outlined in the analysis plan. If the observed result is inconsistent with the null hypothesis, reject it.
\end{enumerate}

Similar to estimation, statistical hypothesis testing does not provide results with 100\% certainty but rather with a certain level of confidence, and errors can occur. Errors can be classified into two types:

\begin{itemize}
 \item \textbf{Type I Error:} Rejecting the null hypothesis $H_0$
       and accepting the alternative hypothesis $H_1$ while $H_0$ is true.
 \item \textbf{Type II Error:} Failing to reject the null hypothesis $H_0$ and not accepting the alternative hypothesis $H_1$ while $H_1$ is true.
\end{itemize}

Both types of errors can have adverse consequences. Depending on the situation, it's assessed which type of error leads to more serious consequences and should be minimized.

To make decisions about rejecting the null hypothesis, statisticians rely on specific rules. These rules are listed in the analysis plan. Traditionally, statisticians describe these decision rules in two ways: referencing a p-value or referencing a region of acceptance.

\begin{itemize}
 \item \textbf{$p$-value:} The strength of evidence in favor of a null hypothesis is measured by the $p$-value. Suppose the test statistic is denoted by S. The $p$-value is the probability of observing a test statistic as extreme as $S$, assuming the null hypothesis is true. If the $p$-value is smaller than the significance level, we reject the null hypothesis.
 \item \textbf{Region of Acceptance:} The region of acceptance is a range of values. If the sample statistic falls within the region of acceptance, the null hypothesis is not rejected. The region of acceptance is set so that the probability of committing a Type I error is equivalent to the significance level.
\end{itemize}

Values outside the region of acceptance are called the region of rejection. If the sample statistic falls within the region of rejection, the null hypothesis is rejected. In such cases, it's said that the null hypothesis has been rejected at the  significance level.

\section{Linear Regression}
Linear regression constructs a linear model between independent variables $\mathbf{x}=(0, x_1,\ldots, x_d)\in\RR^{d+1}$ and a dependent variable $y\in\RR$. Traditionally, the relation is expressed as

\begin{equation}
 \begin{aligned}
  y = w_0 + \sum\limits_{i=1}^n w_ix_i = \mathbf{w}^\top\mathbf{x},
 \end{aligned}
\end{equation}

where $\{w_i\}_{i=0}^d$ are parameters to learn. Let $\{(\mathbf{x}_n, \hat{y}_n)\}_{n=1}^N$ be collected observations. Our objective is to minimize the error
\begin{equation}
 \begin{aligned}
  \L(\mathbf{w})
   & = \sum\limits_{n=1}^N (y-\hat{y})^2                                                              \\
   & = \sum\limits_{n=1}^N (\mathbf{w}^\top \mathbf{x}_n-\hat{y}_n)^2                                 \\
   & = \|\mathbf{X} \mathbf{w} - \hat{\mathbf{y}}\|^2                                               ,
 \end{aligned}
\end{equation}
where $\hat{\mathbf{y}} = (\hat{y}_1,\ldots,\hat{y}_n)^\top \in \RR^n$ and $\mathbf{X} = (\mathbf{x}_1,\ldots,\mathbf{x}_n)\in\RR^{n\times(d+1)}$.
Taking the derivative of $\L$, we have
$$\dfrac{\d\L(\mathbf{w})}{\d \mathbf{w}} = 2\mathbf{X}^\top(\mathbf{X} \mathbf{w} - \hat{\mathbf{y}}).$$

Equivalently, $\mathbf{X}^\top \mathbf{X} \mathbf{w} =  \mathbf{X}^\top\hat{\mathbf{y}}$. If $\mathbf{X}^\top \mathbf{X}$ is invertible i.e. the independent variables are linearly independent, we have the unique solution
\begin{equation}
 \mathbf{w} = (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\hat{\mathbf{y}}.
\end{equation}

Linear regression can be formulated extensively in a probabilistic way. Let $\epsilon\sim\N(0,\sigma^2)$ be a normal-distributed noise, we have the model

\begin{equation}
 y = \mathbf{w}^\top\mathbf{x} + \epsilon \sim \N(\mathbf{w}^\top\mathbf{x}, \sigma^2).
\end{equation}

Hence, for each data $\mathbf{x}_n, n=1,\ldots, N$, the probability density of $\hat{y}_n$ given $\mathbf{w}$ and $\sigma^2$ is
\begin{equation}
 p(\hat{y}_n | \mathbf{x}_n, \mathbf{w}, \sigma^2) = \dfrac{1}{\sqrt{2\pi}\sigma}\exp\left(-\dfrac{1}{2\sigma^2}(\hat{y}_n - \mathbf{w}^\top\mathbf{x})^2\right).
\end{equation}

Maximizing the likelihood $\prod\limits_{i=1}^N p(\hat{y}_n | \mathbf{x}_n, \mathbf{w}, \sigma^2)$ yields the same solution $\mathbf{w}$ as the original linear regression. Additionally,
\begin{equation}
 \sigma^2 = \dfrac{1}{N}\sum\limits_{i=1}^N(\hat{y}_n - \mathbf{w}^\top\mathbf{x})^2.
\end{equation}

In summary, some requirements need to be satisfies for an accurate and reliable solution.

\begin{enumerate}
 \item  The independent and dependent variables correlate. This means that the change in one or more independent variables induces the change in the dependent variable.
 \item The observed data in the dataset are independent of each other. This means that the value of the dependent variable for one observed data point does not depend on the value of the dependent variable for another observed data point.
 \item Across all levels of the independent variables, the variance of the errors remains constant. This indicates that the number of independent variables does not affect the variability of the errors.
 \item The errors in the model follow a normal distribution.
\end{enumerate}

% \subsubsection{Hai loại hồi quy tuyến tính}

% \textbf{- Hồi quy tuyến tính đơn: }\\
% + Nếu một biến độc lập duy nhất được sử dụng để dự đoán giá trị của một biến phụ thuộc số, thì thuật toán hồi quy tuyến tính như vậy được gọi là Hồi quy tuyến tính đơn.\\
% + Giá trị kỳ vọng của $Y$ ở mỗi mức $x$ là biến ngẫu nhiên $E(Y|x) = \beta_0 + \beta_{1}x$. Ta cho rằng mỗi biến $Y$ có thể được mô tả bằng mô hình $Y = \beta_0 + \beta_{1}x + \varepsilon$.\\
% + Trong đó: $Y$ là biến độc lập, $\beta_0$ là giao điểm của $Y$ khi $x$ bằng 0, $\beta_1$ là hệ số của $x$, $\varepsilon$ là hệ số độc lập của mô hình.\\
% + Tức là $Y_i = \beta_0 + \beta_{1}x_i + \varepsilon_i $ với $i=1,2,...,n$.\\
% + Để đạt được đường hồi quy phù hợp nhất, điều cần thiết là phải đưa ra dự đoán giá trị mà sự khác biệt giữa giá trị dự đoán và giá trị thực $Y$ là nhỏ nhất. Vì vậy, chúng ta phải thay đổi giá trị của $\beta_0,  \beta_{1}$ , để đạt được giá trị tốt nhất của cả hai.

% \textbf{- Hồi quy tuyến tính bội:}\\
% + Nếu sử dụng nhiều hơn một biến độc lập để dự đoán giá trị của một biến phụ thuộc số lượng thì thuật toán hồi quy tuyến tính như vậy được gọi là Đa tuyến tính hồi quy.\\
% + Giá trị kỳ vọng của $Y$ ở mỗi mức $x$ là biến ngẫu nhiên $E(Y|x) = \beta_0 + \beta_{1}x_1 + ... + \beta_{i}x_i$. Ta cho rằng mỗi biến $Y$,
% có thể được mô tả bằng mô hình $Y = \beta_0 + \beta_{1}x_1 + ... + \beta_{i}x_i \varepsilon_i $.\\
% + Trong đó: $Y$ là biến độc lập, $\beta_0$ là giao điểm của $Y$ khi $x$ bằng 0, $\beta_1$ là hệ số của $x$, $\varepsilon$ là hệ số độc lập của mô hình.\\
% + Tức là $Y_i = \beta_0 + \beta_{1}x_1 + ... + \beta_{i}x_i  \varepsilon_i $ với $i=1,2,...,n$.\\

% \subsubsection{Định nghĩa hồi quy từng bước}

% - Hồi quy từng bước là một phương pháp điều chỉnh mô hình hồi quy bằng cách lặp đi lặp lại, thêm hoặc loại bỏ các biến. Nó được sử dụng để xây dựng một mô hình chính xác và chi tiết, có nghĩa là nó có số lượng biến nhỏ nhất có thể giải thích dữ liệu.

% - Có hai loại hồi quy từng bước là:\\
% + Lựa chọn chuyển tiếp: Thuật toán bắt đầu với một mô hình trống và lặp đi lặp lại hoặc thêm các biến vào mô hình cho đến khi không có cải tiến nào nữa.\\
% + Loại bỏ ngược: Thuật toán bắt đầu với một mô hình bao gồm tất cả các biến và loại bỏ lặp đi lặp lại các biến cho đến khi không còn cải tiến nào nữa.\\
% + Ưu điểm của hồi quy từng bước là nó có thể tự động chọn các biến quan trọng nhất cho mô hình và xây dựng một mô hình chi tiết. Điểm bất lợi là nó không phải lúc nào cũng chọn được mô hình tốt nhất và có thể nhạy cảm với thứ tự các biến được thêm vào hoặc loại bỏ.
% \subsubsection{Hồi quy từng bước và hồi quy tuyến tính}

% - Hồi quy tuyến tính là một phương pháp thống kê được sử dụng để mô hình hóa mối quan hệ giữa một biến phụ thuộc và một hoặc nhiều biến độc lập bằng cách khớp phương trình tuyến tính với dữ liệu quan sát được. Nói cách khác, đây là phương pháp dự đoán phản hồi (hoặc biến phụ thuộc) dựa trên một hoặc nhiều biến dự đoán.

% - Hồi quy từng bước là phương pháp xây dựng mô hình hồi quy bằng cách thêm hoặc loại bỏ các yếu tố dự đoán theo kiểu từng bước. Mục tiêu của hồi quy từng bước là xác định tập hợp con các yếu tố dự đoán mang lại hiệu suất dự đoán tốt nhất cho biến phản hồi. Điều này được thực hiện bằng cách bắt đầu với một mô hình trống và lặp đi lặp lại việc thêm hoặc loại bỏ các yếu tố dự đoán dựa trên mức độ mối quan hệ của chúng với biến phản hồi.

% - Tóm lại, hồi quy tuyến tính là phương pháp mô hình hóa mối quan hệ giữa phản hồi và một hoặc nhiều biến dự đoán, trong khi hồi quy từng bước là phương pháp xây dựng mô hình hồi quy bằng cách lặp đi lặp lại thêm hoặc loại bỏ các yếu tố dự đoán.

\section{Analysis of Variance}

Analysis of Variance (ANOVA) is a statistical analysis tool used to partition the total observed variability found in a dataset into two parts: Systematic factors and Random factors. Systematic factors statistically influence a certain dataset, while random factors do not. We use ANOVA to determine the influence of independent variables on the dependent variable in regression studies.

The $t$-test and $z$-test methods, developed in the 20th century, were used for statistical analysis until 1918 when Ronald Fisher introduced the Analysis of Variance method. Both serve as hypothesis tests to assess whether there is any significant difference between means. The $t$-test is used when the population variance is unknown or the sample size is small $(n < 30)$. Meanwhile, the $z$-test is applied when the population variance is known and the sample size is large $(n > 30)$. The $t$-test uses the Student's t-distribution, while the $z$-test uses the standard normal distribution. As the sample size increases, the T-distribution converges to the standard normal distribution. The $t$-test can be understood as a statistical test used to compare and analyze whether the mean values of two populations are different when the standard deviation is unknown. Conversely, the $z$-test is a parametric test applied when the standard deviation is known to determine whether the means of two datasets are different. ANOVA is also known as Fisher's analysis of variance and is an extension of both $t$-tests and $z$-tests.

\subsection{ANOVA Usecases}

ANOVA test is the initial step in analyzing factors influencing a specific dataset. Upon completing a test, additional checks on measurable methodological factors contributing to dataset inconsistency are conducted. The results, using $F$-distribution, can be utilized to generate supplementary data suitable for proposed regression models. The test enables comparison of more than two groups simultaneously to determine whether a relationship exists among them. The outcome of ANOVA analysis, the F-statistic (also known as the F-ratio), allows for analysis across multiple datasets to ascertain variation between samples and within samples.

If no significant differences exist among the tested groups (known as the null hypothesis), the $F$-statistic result of ANOVA will be close to $1$. The distribution of all possible values of the F-statistic follows the $F$-distribution. This is a family of probability distributions characterized by two parameters, known as the degrees of freedom of the numerator and the degrees of freedom of the denominator.

\subsection{Types of ANOVA}

There are two main types of ANOVA: one-way ANOVA and two-way ANOVA. Additionally, there are variations of ANOVA such as  Multivariate ANOVA (MANOVA), which differs from ANOVA in that the former tests multiple dependent variables simultaneously while the latter evaluates only one dependent variable at a time. One-way or two-way refers to the number of independent variables in the analysis of variance. Furthermore, ANOVA relies on assumptions. ANOVA tests assume that the data are normally distributed, the variance is nearly equal across groups, and all observations are independent. If these assumptions are not met, ANOVA may not be useful for group comparisons.

\subsubsection{One-way ANOVA}
One-way ANOVA is used to assess the effect of a single factor on a single response variable. It determines whether all samples are similar i.e. determine whether there is a statistically significant difference between the means of three or more independent (unrelated) groups. One-way ANOVA is formulated as
\begin{equation}
 Y_{i,j}=\mu_j + \varepsilon_{i,j},
\end{equation}
where $i=1,\ldots,n_j$ are indices of observations within the same group, $j=1,\ldots,k$ are indices of influenced groups, $\mu_j$ s the mean for each affected group and $\varepsilon_{i,j} \sim N(0,\sigma^2)$ is normal-distributed errors, independent for all $i$ and $j$. Mean and variance of $Y_{i,j}$ are $E(Y_{i,j})=\mu_j$ and $Var(Y_{i,j})=\sigma^2$, respectively.

\subsubsection{Two-way ANOVA}

Two-way ANOVA is an extension of one-way ANOVA, where there are two independent factors. It is used to observe the interaction between two factors and test the effects of two factors simultaneously. It is formulated as
\begin{equation}
 Y_{i,j}=\mu_j+\gamma_j+(\mu\gamma){i,j}+\varepsilon{i,j,k}.
\end{equation}
Assumptions about random variables are similar to one-way ANOVA.
